# -*- coding: utf-8 -*-
"""Test1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t50LO2QafhSFSSesfAhqHbAr5XiDlutv
"""

!pip install requests aiohttp pandas tqdm pyyaml loguru typer python-dotenv biopython

from google.colab import drive
drive.mount('/content/drive')

import os

# Define project folder
project_path = "/content/drive/My Drive/pubmed_fetcher"

# Create project folder if it doesn't exist
os.makedirs(project_path, exist_ok=True)

print(f"‚úÖ Project folder created at: {project_path}")

api_key = "72201812cd102ae33b384a0a821790efad08"  # Replace with your actual API key

# Save API key in a .env file inside Google Drive
env_file_path = "/content/drive/My Drive/pubmed_fetcher/.env"
with open(env_file_path, "w") as f:
    f.write(f"PUBMED_API_KEY={api_key}\n")

print(f"‚úÖ API Key stored securely at {env_file_path}")

from dotenv import load_dotenv
import os

# Load API key from .env file
load_dotenv("/content/drive/My Drive/pubmed_fetcher/.env")

API_KEY = os.getenv("PUBMED_API_KEY")
print(f"‚úÖ API Key Loaded: {API_KEY[:5]}********")  # Masking for security

import requests

BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"

def fetch_pubmed_ids(query, max_results=100):
    """Fetch PubMed IDs based on a search query."""
    params = {
        "db": "pubmed",
        "term": query,
        "retmode": "json",
        "retmax": max_results,
    }
    if API_KEY:
        params["api_key"] = API_KEY

    response = requests.get(BASE_URL, params=params)
    if response.status_code == 200:
        data = response.json()
        return data["esearchresult"]["idlist"]
    else:
        print("‚ùå Error:", response.status_code, response.text)
        return []

# Example search query
query = "Bioscience"
pubmed_ids = fetch_pubmed_ids(query)
print("‚úÖ PubMed IDs Retrieved:", pubmed_ids)

def fetch_article_details(pubmed_ids):
    """Fetch detailed information for a list of PubMed IDs."""
    details_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi"
    params = {
        "db": "pubmed",
        "id": ",".join(pubmed_ids),
        "retmode": "json",
    }
    if API_KEY:
        params["api_key"] = API_KEY

    response = requests.get(details_url, params=params)
    if response.status_code == 200:
        data = response.json()
        articles = []
        for pubmed_id in pubmed_ids:
            article = data["result"].get(pubmed_id, {})
            articles.append({
                "PubmedID": pubmed_id,
                "Title": article.get("title", "N/A"),
                "PublicationDate": article.get("pubdate", "N/A"),
            })
        return articles
    else:
        print("‚ùå Error:", response.status_code, response.text)
        return []

# Fetch details for the retrieved PubMed IDs
articles_data = fetch_article_details(pubmed_ids)

# Print sample output
print("‚úÖ Sample Article Details:", articles_data[0] if articles_data else "No data")

file_path = "/content/drive/My Drive/pubmed_fetcher/pubmed_articles.txt"

# Save the articles
with open(file_path, "w") as file:
    for article in articles_data:
        file.write(f"PMID: {article['PubmedID']}\n")
        file.write(f"Title: {article['Title']}\n")
        file.write(f"Publication Date: {article['PublicationDate']}\n")
        file.write("\n")  # Add space between articles

print(f"‚úÖ Articles saved to {file_path}")

def fetch_authors_affiliations(pubmed_ids):
    """Fetch authors and affiliations for PubMed articles."""
    details_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    params = {
        "db": "pubmed",
        "id": ",".join(pubmed_ids),
        "retmode": "xml",
    }
    if API_KEY:
        params["api_key"] = API_KEY

    response = requests.get(details_url, params=params)
    if response.status_code == 200:
        from bs4 import BeautifulSoup  # Importing inside function to avoid unused imports
        soup = BeautifulSoup(response.text, "xml")
        articles = []

        for article in soup.find_all("PubmedArticle"):
            pubmed_id = article.find("PMID").text if article.find("PMID") else "N/A"
            title = article.find("ArticleTitle").text if article.find("ArticleTitle") else "N/A"
            publication_date = article.find("PubDate").text if article.find("PubDate") else "N/A"

            authors = []
            for author in article.find_all("Author"):
                last_name = author.find("LastName").text if author.find("LastName") else ""
                first_name = author.find("ForeName").text if author.find("ForeName") else ""
                affiliation = author.find("AffiliationInfo/Affiliation").text if author.find("AffiliationInfo/Affiliation") else "N/A"
                authors.append({"Name": f"{first_name} {last_name}".strip(), "Affiliation": affiliation})

            articles.append({
                "PubmedID": pubmed_id,
                "Title": title,
                "PublicationDate": publication_date,
                "Authors": authors
            })

        return articles
    else:
        print("‚ùå Error:", response.status_code, response.text)
        return []

# Fetch author details
articles_with_authors = fetch_authors_affiliations(pubmed_ids)

# Print a sample output
print("‚úÖ Sample Article with Authors:", articles_with_authors[0] if articles_with_authors else "No data")

import json

# Save the articles
file_path = "/content/drive/My Drive/pubmed_fetcher/pubmed_articles_with_authors.json"
with open(file_path, "w") as file:
    json.dump(articles_with_authors, file, indent=4)

print(f"‚úÖ Articles with authors saved to {file_path}")

config_text = """non_academic_keywords:
  - "Inc"
  - "Ltd"
  - "Pharma"
  - "Biotech"
  - "Corporation"
  - "Company"
  - "LLC"
  - "Laboratories"
  - "Technologies"
  - "Research Institute"
  - "Healthcare"
  - "Biosciences"

non_academic_email_patterns:
  - ".com"
  - ".net"
  - ".org"

academic_email_patterns:
  - ".edu"
  - ".ac."
  - "university"
  - "college"
"""

# Save config.yaml in Google Drive
config_path = "/content/drive/My Drive/pubmed_fetcher/config.yaml"
with open(config_path, "w") as file:
    file.write(config_text)

print(f"‚úÖ config.yaml file created at {config_path}")

import yaml

# Load YAML file
with open(config_path, "r") as file:
    config = yaml.safe_load(file)

# Print rules to verify
print("üîπ Non-Academic Keywords:", config["non_academic_keywords"])
print("üîπ Non-Academic Email Patterns:", config["non_academic_email_patterns"])
print("üîπ Academic Email Patterns:", config["academic_email_patterns"])

def is_non_academic_author(affiliation):
    """Determines if an author is non-academic based on affiliation."""

    # Load YAML configuration
    with open(config_path, "r") as file:
        config = yaml.safe_load(file)

    # Check affiliation keywords
    for keyword in config["non_academic_keywords"]:
        if keyword.lower() in affiliation.lower():
            return True  # Non-academic author

    return False  # Default: Academic author

import json

# Load the saved articles
with open("/content/drive/My Drive/pubmed_fetcher/pubmed_articles_with_authors.json", "r") as file:
    articles = json.load(file)

non_academic_authors = []

# Process each article
for article in articles:
    for author in article["Authors"]:
        if is_non_academic_author(author["Affiliation"]):
            non_academic_authors.append({
                "PubmedID": article["PubmedID"],
                "Title": article["Title"],
                "PublicationDate": article["PublicationDate"],
                "Author": author["Name"],
                "Affiliation": author["Affiliation"]
            })

# Print sample output
print("‚úÖ Sample Non-Academic Author:", non_academic_authors[0] if non_academic_authors else "No data")

import pandas as pd

# Convert to DataFrame
df = pd.DataFrame(non_academic_authors)

# Save as CSV
filtered_csv_path = "/content/drive/My Drive/pubmed_fetcher/non_academic_authors.csv"
df.to_csv(filtered_csv_path, index=False)

print(f"‚úÖ Filtered authors saved to {filtered_csv_path}")

!wget -q ftp://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/edirect.tar.gz
!tar -xf edirect.tar.gz
!mv edirect $HOME
!echo 'export PATH=$HOME/edirect:$PATH' >> ~/.bashrc
!source ~/.bashrc

!$HOME/edirect/efetch -version

import os
os.environ["PATH"] += os.pathsep + os.path.expanduser("~/edirect")

import os

# Ensure EDirect is available
os.environ["PATH"] += os.pathsep + os.path.expanduser("~/edirect")

# Query for PubMed Search
query = "Bioscience"
batch_size = 9000  # Number of IDs per batch
retstart = 0       # Start index
batch_number = 1   # File counter
total_ids = 0      # Counter for total IDs fetched

while True:
    print(f"Fetching batch {batch_number} (Starting from {retstart})...")

    # Run EDirect command using os.system
    cmd = f'esearch -db pubmed -query "{query}" | efetch -format uid -retstart {retstart} -retmax {batch_size} > pubmed_ids_{batch_number}.txt'
    os.system(cmd)

    # Check if the file contains any IDs
    file_name = f"pubmed_ids_{batch_number}.txt"
    if os.path.exists(file_name):
        with open(file_name, "r") as f:
            ids = f.readlines()

        num_ids = len(ids)
        if num_ids > 0:
            print(f"‚úÖ Batch {batch_number} saved: {file_name} ({num_ids} IDs)")
            total_ids += num_ids
            retstart += batch_size  # Move to next batch
            batch_number += 1
        else:
            print("üî¥ No more IDs detected. Stopping.")
            os.remove(file_name)  # Remove empty file
            break  # Stop loop
    else:
        print("‚ö†Ô∏è Error: File not found. Stopping.")
        break

print(f"‚úÖ All batches fetched successfully! Total PubMed IDs collected: {total_ids}")

from Bio import Entrez

# Define search query
QUERY = "biotech industry OR pharmaceutical research"
BATCH_SIZE = 9000  # Max IDs to fetch in one request
EMAIL = "soumendraassassin@gmail.com"  # Set your email (required for Entrez API)

# Set Entrez email
Entrez.email = EMAIL

# Function to get the total number of PubMed articles matching the query
def get_total_pubmed_count(query):
    handle = Entrez.esearch(db="pubmed", term=query, retmax=1)  # Fetch only count, not IDs
    record = Entrez.read(handle)
    handle.close()
    return int(record["Count"])  # Convert count to integer

# Function to fetch PubMed IDs (up to BATCH_SIZE)
def fetch_pubmed_ids(query, batch_size):
    handle = Entrez.esearch(db="pubmed", term=query, retmax=batch_size)
    record = Entrez.read(handle)
    handle.close()
    return record["IdList"]

# Get total PubMed count
total_count = get_total_pubmed_count(QUERY)
print(f"üîç Total PubMed articles found for query: {total_count}")

# Get PubMed IDs
pubmed_ids = fetch_pubmed_ids(QUERY, BATCH_SIZE)

# Save IDs to a file
with open("pubmed_ids.txt", "w") as f:
    f.write("\n".join(pubmed_ids))

print(f"‚úÖ Successfully fetched {len(pubmed_ids)} PubMed IDs and saved to 'pubmed_ids.txt'.")

from Bio import Entrez
import time
import re
import os

# Set email for Entrez API
Entrez.email = "soumendraassassin@gmail.com"

# Load stored PubMed IDs from file
with open("pubmed_ids.txt", "r") as f:
    pubmed_ids = f.read().splitlines()

# Expanded keyword lists for better classification
ACADEMIC_KEYWORDS = [
    "university", "college", "institute", "hospital", "research center",
    "faculty", "department", "academy", "school", "medical center", "clinic",
    "health system", "foundation", "national institute", "public health"
]
NON_ACADEMIC_KEYWORDS = [
    "company", "inc.", "ltd.", "corporation", "startup", "llc", "private",
    "gmbh", "s.a.", "s.r.l.", "corp", "pvt", "s.p.a", "enterprises", "solutions",
    "group", "technologies", "consulting", "venture", "biotech", "pharma",
    "industries", "business", "financial", "commercial"
]

# Function to check if an affiliation is non-academic
def is_non_academic(affiliation):
    affiliation = affiliation.lower()  # Convert to lowercase for case-insensitivity
    return any(re.search(rf"\b{keyword}\b", affiliation) for keyword in NON_ACADEMIC_KEYWORDS)

# Function to check if an affiliation is academic
def is_academic(affiliation):
    affiliation = affiliation.lower()  # Convert to lowercase
    return any(re.search(rf"\b{keyword}\b", affiliation) for keyword in ACADEMIC_KEYWORDS)

# Function to fetch author details in batch mode (with error handling)
def fetch_author_details_batch(pubmed_ids):
    try:
        handle = Entrez.efetch(db="pubmed", id=",".join(pubmed_ids), rettype="xml")
        records = Entrez.read(handle)
        handle.close()

        non_academic_authors = []
        for article in records.get("PubmedArticle", []):  # Ensure "PubmedArticle" exists
            pmid = article["MedlineCitation"].get("PMID", "Unknown")

            # Check if "Article" and "AuthorList" exist
            if not article["MedlineCitation"].get("Article") or not article["MedlineCitation"]["Article"].get("AuthorList"):
                continue  # Skip if missing

            for author in article["MedlineCitation"]["Article"]["AuthorList"]:
                # Ensure "AffiliationInfo" exists and is non-empty
                if "AffiliationInfo" in author and author["AffiliationInfo"]:
                    affiliation = author["AffiliationInfo"][0].get("Affiliation", "").strip()

                    # Skip empty affiliations
                    if not affiliation:
                        continue

                    # Check if affiliation is non-academic
                    if is_non_academic(affiliation) and not is_academic(affiliation):
                        last_name = author.get("LastName", "Unknown")
                        non_academic_authors.append((pmid, last_name, affiliation))

        return non_academic_authors
    except Exception as e:
        print(f"‚ö†Ô∏è Error fetching batch: {e}")
        return []

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define Google Drive path for saving the file
drive_folder_path = "/content/drive/My Drive/pubmed_fetcher"
os.makedirs(drive_folder_path, exist_ok=True)  # Create folder if it doesn't exist

# Process all 9000 PubMed IDs in batches of 200 to avoid API limits
BATCH_SIZE = 200
all_non_academic_authors = []

for i in range(0, len(pubmed_ids), BATCH_SIZE):
    batch = pubmed_ids[i:i + BATCH_SIZE]
    print(f"üîç Processing batch {i//BATCH_SIZE + 1}/{len(pubmed_ids)//BATCH_SIZE + 1}...")

    non_academic_authors = fetch_author_details_batch(batch)
    all_non_academic_authors.extend(non_academic_authors)

    time.sleep(1)  # Avoid hitting NCBI API rate limits

# Save non-academic authors to Google Drive
output_file_path = os.path.join(drive_folder_path, "non_academic_authors.txt")

with open(output_file_path, "w") as f:
    for entry in all_non_academic_authors:
        f.write("\t".join(entry) + "\n")

print(f"‚úÖ Found {len(all_non_academic_authors)} non-academic authors. Saved to '{output_file_path}'.")

import csv

# Define input and output file paths
input_file_path = "/content/drive/My Drive/pubmed_fetcher/non_academic_authors.txt"  # Update this if using different path
output_file_path = "/content/drive/My Drive/pubmed_fetcher/non_academic_authors.csv"

# Define the expected CSV column headers
csv_headers = ["PubmedID", "Title", "Publication Date", "Non-academic Author(s)", "Company Affiliation(s)", "Corresponding Author Email"]

# Open the input text file and process its contents
data = []
with open(input_file_path, "r", encoding="utf-8") as file:
    record = {}
    for line in file:
        line = line.strip()
        if not line:
            continue
        if line.startswith("PubmedID:"):
            if record:
                data.append(record)
                record = {}
            record["PubmedID"] = line.split(":", 1)[1].strip()
        elif line.startswith("Title:"):
            record["Title"] = line.split(":", 1)[1].strip()
        elif line.startswith("Publication Date:"):
            record["Publication Date"] = line.split(":", 1)[1].strip()
        elif line.startswith("Non-academic Author(s):"):
            record["Non-academic Author(s)"] = line.split(":", 1)[1].strip()
        elif line.startswith("Company Affiliation(s):"):
            record["Company Affiliation(s)"] = line.split(":", 1)[1].strip()
        elif line.startswith("Corresponding Author Email:"):
            record["Corresponding Author Email"] = line.split(":", 1)[1].strip()

    if record:
        data.append(record)  # Append last record

# Write the extracted data to a CSV file
with open(output_file_path, "w", newline="", encoding="utf-8") as csvfile:
    writer = csv.DictWriter(csvfile, fieldnames=csv_headers)
    writer.writeheader()
    writer.writerows(data)

print(f"CSV file created successfully: {output_file_path}")

# Provide a download link in Google Colab
from google.colab import files
files.download(output_file_path)

import argparse
import pandas as pd
import sys

def read_csv(file_path, search_term=None):
    try:
        df = pd.read_csv("/content/drive/My Drive/pubmed_fetcher/sample_out.csv")

        if df.empty:
            print("The CSV file is empty.")
            sys.exit(1)

        print(f"\nüìÇ Loaded {len(df)} records from '{file_path}'\n")

        if search_term:
            filtered_df = df[df.apply(lambda row: row.astype(str).str.contains(search_term, case=False).any(), axis=1)]
            if filtered_df.empty:
                print(f"No records found for search: '{search_term}'")
            else:
                print(f"üîç Showing results for '{search_term}':\n")
                print(filtered_df)
        else:
            print(df.head(10))  # Show first 10 rows

    except Exception as e:
        print(f"‚ùå Error reading CSV: {e}")
        sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description="CLI tool to read and filter PubMed CSV files.")
    parser.add_argument("file", type=str, help="Path to the CSV file")
    parser.add_argument("-s", "--search", type=str, help="Search term for filtering", default=None)

    args = parser.parse_args()

    read_csv(args.file, args.search)

if __name__ == "__main__":
    main()

!git config --global user.name "soumendra0005"
!git config --global user.email "soumendraassassin@gmail.com"

!git clone https://github.com/soumendra0005/pubmed_fetcher

# Commented out IPython magic to ensure Python compatibility.
# %cd pubmed_fetcher/

!rsync -av --exclude='*.gsheet' "/content/drive/My Drive/pubmed_fetcher/" /content/pubmed_fetcher/

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/pubmed_fetcher  # Ensure we are inside the repo

!git add .
!git commit -m "Pushed all files from Google Drive pubmed_fetcher"
!git branch -M main
!git remote add origin https://github.com/soumendra0005/pubmed_fetcher.git
!git push -u origin main

!zip -r /content/pubmed_fetcher.zip /content/pubmed_fetcher

from google.colab import files
files.download("/content/pubmed_fetcher.zip")